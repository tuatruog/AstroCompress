# Executable for evaluating trained models.
# e.g., python -m ar.eval --workdir expms/214971/wid=0-pixelcnn-data=lco-split_bits=0-model=pixelcnn-nr_resnet=5-nr_filters=160-nr_mix=12 --dataset lco
# Based on https://github.com/mandt-lab/shallow-ntc/blob/master/eval.py

from absl import app
from absl import flags
from absl import logging
from ml_collections import ConfigDict
import torch
from ar.common.eval_lib import eval_workdir
import ar.common.data_lib as data_lib
from ar.common.experiment import load_workdir_config
import pprint

FLAGS = flags.FLAGS

# config_flags.DEFINE_config_file('config', None, 'File path to the eval configuration.',
#                                 lock_config=True)
flags.DEFINE_string('workdir', None, "workdir to evaluate. This is generated by train_lib.")
flags.DEFINE_string('dataset', None, 'Dataset to eval.')
flags.DEFINE_integer('batch_size', 1, 'Size of eval data batches.')
flags.DEFINE_integer('patch_size', None, 'Size of cropped patches (defaults to model.config.eval_data.patch_size).')
flags.DEFINE_string('results_dir', None, 'Directory to store results.')
flags.DEFINE_boolean('skip_existing', True, 'Set to False to overwrite existing results files.')


def main(argv):
  if len(argv) > 1:
    raise app.UsageError('Too many command-line arguments.')

  # Initialize to the eval_data config from the training experiment
  # (which has the correct data format setting (split_bits_axis))
  expm_config = load_workdir_config(FLAGS.workdir)
  data_config = expm_config.eval_data
  # Override based on cmdline args.
  data_config.data_spec = FLAGS.dataset
  data_config.batch_size = FLAGS.batch_size
  data_config.random_crop = False

  data_config.pin_memory = True
  data_config.num_workers = 1
  
  logging.info("Eval data config:\n%s", pprint.pformat(data_config))

  eval_iter = data_lib.get_data_iter(data_config, subset='test')


  results_dir = FLAGS.results_dir
  if results_dir is None:
    results_dir = f"./results/{FLAGS.dataset}/"

  model_evaluate_kwargs = dict(patch_size=FLAGS.patch_size)
  results_file_path = eval_workdir(FLAGS.workdir, eval_iter, results_dir,
                                   skip_existing=FLAGS.skip_existing, **model_evaluate_kwargs)

  return results_file_path


if __name__ == '__main__':
  flags.mark_flags_as_required(['workdir', 'dataset'])
  app.run(main)
